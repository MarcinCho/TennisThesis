{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28870 entries, 0 to 28869\n",
      "Data columns (total 25 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   win_percentage_vs            28870 non-null  float64\n",
      " 1   bp_factor_vs                 28870 non-null  float64\n",
      " 2   first_won_serve_vs           28870 non-null  float64\n",
      " 3   second_won_serve_vs          28870 non-null  float64\n",
      " 4   ace_probability_vs           28870 non-null  float64\n",
      " 5   double_fault_probability_vs  28870 non-null  float64\n",
      " 6   aceDf_vs                     28870 non-null  float64\n",
      " 7   rank                         28870 non-null  float64\n",
      " 8   rank_points                  28870 non-null  float64\n",
      " 9   elo                          28870 non-null  float64\n",
      " 10  glicko                       28870 non-null  float64\n",
      " 11  ht                           28870 non-null  float64\n",
      " 12  games_played                 28870 non-null  int64  \n",
      " 13  win_percentage               28870 non-null  float64\n",
      " 14  surface_wins                 28870 non-null  float64\n",
      " 15  bp_factor                    28870 non-null  float64\n",
      " 16  first_won_serve              28870 non-null  float64\n",
      " 17  second_won_serve             28870 non-null  float64\n",
      " 18  double_fault_probability     28870 non-null  float64\n",
      " 19  aceDf                        28870 non-null  float64\n",
      " 20  points_on_return             28870 non-null  float64\n",
      " 21  serve_points_won             28870 non-null  float64\n",
      " 22  completeness                 28870 non-null  float64\n",
      " 23  total_serve_points           28870 non-null  float64\n",
      " 24  ace_probability              28870 non-null  float64\n",
      "dtypes: float64(24), int64(1)\n",
      "memory usage: 5.5 MB\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import RFECV\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, time, timedelta\n",
    "\n",
    "df = pd.read_csv('../db/out/wta_s.csv')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_2023 = df[df['date'].str.contains(\n",
    "    '2023', na=False) | df['date'].str.contains('2022', na=False)]\n",
    "\n",
    "df.drop(df_2023.index, inplace=True)\n",
    "\n",
    "non_numeric_columns = df_2023.select_dtypes(['object']).columns\n",
    "\n",
    "df_numeric_only = df_2023.drop(non_numeric_columns, axis=1)\n",
    "\n",
    "df_2023 = df_numeric_only\n",
    "\n",
    "df_2023 = df_2023.drop(['match_id'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df_2023 = df_2023.fillna(df_2023.median())\n",
    "\n",
    "y_2023 = pd.DataFrame(df_2023['y'])\n",
    "x_2023 = df_2023.drop(['y'], axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_2023 = scaler.fit_transform(x_2023)\n",
    "\n",
    "non_numeric_columns = df.select_dtypes(['object']).columns\n",
    "\n",
    "df_numeric_only = df.drop(non_numeric_columns, axis=1)\n",
    "\n",
    "df = df_numeric_only\n",
    "\n",
    "df = df.drop(['match_id'], axis=1)\n",
    "\n",
    "df = df.fillna(df.median())\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = pd.DataFrame(df['y'])\n",
    "df = df.drop(['y'], axis=1)\n",
    "X = df\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.1, random_state=45, stratify=Y)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrivePW\\OneDrive - Politechnika Warszawska\\Documents\\PracaInżynierska\\Software\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trening\n",
      "{'0': {'precision': 0.6438082556591211, 'recall': 0.6752793296089385, 'f1-score': 0.6591683708248125, 'support': 1432.0}, '1': {'precision': 0.6642599277978339, 'recall': 0.6323024054982818, 'f1-score': 0.6478873239436619, 'support': 1455.0}, 'accuracy': 0.6536196744024939, 'macro avg': {'precision': 0.6540340917284775, 'recall': 0.6537908675536102, 'f1-score': 0.6535278473842372, 'support': 2887.0}, 'weighted avg': {'precision': 0.6541155583823034, 'recall': 0.6536196744024939, 'f1-score': 0.6534829107582818, 'support': 2887.0}}\n",
      "Walidacja\n",
      "{'0': {'precision': 0.6174089068825911, 'recall': 0.6389664804469274, 'f1-score': 0.6280027453671928, 'support': 1432.0}, '1': {'precision': 0.6234522942461762, 'recall': 0.6015460295151089, 'f1-score': 0.6123032904148784, 'support': 1423.0}, 'accuracy': 0.6203152364273204, 'macro avg': {'precision': 0.6204306005643836, 'recall': 0.6202562549810182, 'f1-score': 0.6201530178910356, 'support': 2855.0}, 'weighted avg': {'precision': 0.6204210750851766, 'recall': 0.6203152364273204, 'f1-score': 0.6201777630914859, 'support': 2855.0}}\n",
      "AUC:  0.7152632993530303\n",
      "AUC:  0.6435031328886569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrivePW\\OneDrive - Politechnika Warszawska\\Documents\\PracaInżynierska\\Software\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\OneDrivePW\\OneDrive - Politechnika Warszawska\\Documents\\PracaInżynierska\\Software\\.venv\\Lib\\site-packages\\sklearn\\base.py:458: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "d:\\OneDrivePW\\OneDrive - Politechnika Warszawska\\Documents\\PracaInżynierska\\Software\\.venv\\Lib\\site-packages\\sklearn\\base.py:458: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(random_state=45, max_iter=200)\n",
    "\n",
    "\n",
    "result = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "basic_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "print(\"Trening\")\n",
    "print(basic_report)\n",
    "basic_pre = basic_report['weighted avg']['precision']\n",
    "\n",
    "\n",
    "\n",
    "pred_val = model.predict(x_2023)\n",
    "basic_report = classification_report(y_2023, pred_val, output_dict=True)\n",
    "print(\"Walidacja\")\n",
    "print(basic_report)\n",
    "basic_pre_val = basic_report['weighted avg']['precision']\n",
    "\n",
    "auc_basic = metrics.roc_auc_score(y_test, model.predict_proba(X_test)[::, 1])\n",
    "print(\"AUC: \", auc_basic)\n",
    "\n",
    "auc_val = metrics.roc_auc_score(y_2023, model.predict_proba(x_2023)[::, 1])\n",
    "print(\"AUC: \", auc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras import Input\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import History\n",
    "# import keras_tuner as kt\n",
    "from pathlib import Path\n",
    "from time import strftime\n",
    "# from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# def get_run_logdir(root_logdir=\"my_run_logs\"):\n",
    "#     return Path(root_logdir) / strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "\n",
    "# tensorbord_cb = keras.callbacks.TensorBoard(\n",
    "#     get_run_logdir(), profile_batch=(100, 200))\n",
    "\n",
    "\n",
    "# history = History()\n",
    "\n",
    "# # use keras API\n",
    "# tf.random.set_seed(49)\n",
    "\n",
    "\n",
    "# def build_model(hp):\n",
    "#     n_hidden = hp.Int('n_hidden', min_value=1, max_value=4, step=1)\n",
    "#     n_neurons = hp.Int('n_neurons', min_value=10, max_value=65, step=5)\n",
    "#     learning_rate = hp.Float(\"learning_rate\", min_value=0.0001, max_value=0.01,\n",
    "#                              sampling=\"log\")\n",
    "#     kernel_regularizer = hp.Choice('kernel_regularizer', ['l1', 'l2'])\n",
    "#     kernel_initializer = hp.Choice(\n",
    "#         'kernel_initializer', ['he_normal', 'glorot_normal', 'lecun_normal'])\n",
    "#     activation = hp.Choice('activation', ['relu', 'selu', 'sigmoid', 'tanh'])\n",
    "#     optimizer = hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop'])\n",
    "#     if optimizer == 'adam':\n",
    "#         optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "#     elif optimizer == 'sgd':\n",
    "#         optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "#     else:\n",
    "#         optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "#     model = keras.Sequential()\n",
    "#     model.add(Input(shape=X_train.shape[1]))\n",
    "#     for _ in range(n_hidden):\n",
    "#         model.add(Dense(n_neurons, activation=activation,\n",
    "#                   kernel_regularizer=kernel_regularizer, kernel_initializer=kernel_initializer))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "#     model.compile(optimizer=optimizer,\n",
    "#                   loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "# model = tf.keras.Sequential()\n",
    "\n",
    "\n",
    "# random_search_tuner = kt.BayesianOptimization(\n",
    "#     build_model,\n",
    "#     objective='val_accuracy',\n",
    "#     max_trials=25,\n",
    "#     executions_per_trial=3,\n",
    "#     directory='my_dir',\n",
    "#     project_name='wta_ann_bayesian_optimization',\n",
    "#     overwrite=True)\n",
    "\n",
    "# random_search_tuner.search(X_train, y_train, epochs=15,\n",
    "#                            validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "# top3_models = random_search_tuner.get_best_models(num_models=3)\n",
    "# best_model = top3_models[0]\n",
    "\n",
    "# best_trial = random_search_tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "# best_trial.summary()\n",
    "\n",
    "\n",
    "# best_trial.metrics.get_last_value(\"val_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 96)                2496      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 97        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2593 (10.13 KB)\n",
      "Trainable params: 2593 (10.13 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "102/102 [==============================] - 1s 3ms/step - loss: 84.1100 - accuracy: 0.5403 - val_loss: 177.9920 - val_accuracy: 0.3972\n",
      "Epoch 2/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 67.3384 - accuracy: 0.5954 - val_loss: 84.4106 - val_accuracy: 0.4851\n",
      "Epoch 3/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 53.4139 - accuracy: 0.6282 - val_loss: 65.6587 - val_accuracy: 0.5888\n",
      "Epoch 4/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 41.8963 - accuracy: 0.6449 - val_loss: 57.6530 - val_accuracy: 0.6046\n",
      "Epoch 5/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 32.4109 - accuracy: 0.6565 - val_loss: 48.7287 - val_accuracy: 0.6098\n",
      "Epoch 6/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 24.6685 - accuracy: 0.6641 - val_loss: 42.4158 - val_accuracy: 0.6140\n",
      "Epoch 7/100\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 18.4249 - accuracy: 0.6682 - val_loss: 38.5168 - val_accuracy: 0.6224\n",
      "Epoch 8/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 13.4362 - accuracy: 0.6691 - val_loss: 34.6182 - val_accuracy: 0.6207\n",
      "Epoch 9/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 9.5424 - accuracy: 0.6698 - val_loss: 29.0781 - val_accuracy: 0.6214\n",
      "Epoch 10/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 6.5603 - accuracy: 0.6721 - val_loss: 23.7516 - val_accuracy: 0.6196\n",
      "Epoch 11/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 4.3621 - accuracy: 0.6717 - val_loss: 17.5886 - val_accuracy: 0.6200\n",
      "Epoch 12/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 2.8246 - accuracy: 0.6730 - val_loss: 12.0556 - val_accuracy: 0.6214\n",
      "Epoch 13/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.8124 - accuracy: 0.6721 - val_loss: 8.2264 - val_accuracy: 0.6214\n",
      "Epoch 14/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.1814 - accuracy: 0.6728 - val_loss: 6.1111 - val_accuracy: 0.6221\n",
      "Epoch 15/100\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 0.8542 - accuracy: 0.6719 - val_loss: 4.0539 - val_accuracy: 0.6203\n",
      "Epoch 16/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7402 - accuracy: 0.6673 - val_loss: 2.8229 - val_accuracy: 0.6203\n",
      "Epoch 17/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7217 - accuracy: 0.6602 - val_loss: 2.1469 - val_accuracy: 0.6203\n",
      "Epoch 18/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7186 - accuracy: 0.6502 - val_loss: 1.5527 - val_accuracy: 0.6207\n",
      "Epoch 19/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7169 - accuracy: 0.6174 - val_loss: 1.0131 - val_accuracy: 0.6214\n",
      "Epoch 20/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7162 - accuracy: 0.5739 - val_loss: 0.8010 - val_accuracy: 0.6249\n",
      "Epoch 21/100\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 0.7160 - accuracy: 0.5593 - val_loss: 0.7613 - val_accuracy: 0.6186\n",
      "Epoch 22/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7160 - accuracy: 0.5425 - val_loss: 0.9722 - val_accuracy: 0.6256\n",
      "Epoch 23/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7160 - accuracy: 0.5483 - val_loss: 0.8152 - val_accuracy: 0.6252\n",
      "Epoch 24/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7160 - accuracy: 0.5514 - val_loss: 1.0451 - val_accuracy: 0.6210\n",
      "Epoch 25/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7160 - accuracy: 0.5514 - val_loss: 0.6716 - val_accuracy: 0.6249\n",
      "Epoch 26/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7160 - accuracy: 0.5606 - val_loss: 0.8101 - val_accuracy: 0.6231\n",
      "Epoch 27/100\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 0.7160 - accuracy: 0.5555 - val_loss: 0.7114 - val_accuracy: 0.6294\n",
      "Epoch 28/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7160 - accuracy: 0.5534 - val_loss: 0.7521 - val_accuracy: 0.6228\n",
      "Epoch 29/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7160 - accuracy: 0.5594 - val_loss: 0.7636 - val_accuracy: 0.6238\n",
      "Epoch 30/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7160 - accuracy: 0.5655 - val_loss: 0.8284 - val_accuracy: 0.6221\n",
      "Epoch 31/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7160 - accuracy: 0.5447 - val_loss: 0.7366 - val_accuracy: 0.6221\n",
      "Epoch 32/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7160 - accuracy: 0.5554 - val_loss: 0.7707 - val_accuracy: 0.6210\n",
      "Epoch 33/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7160 - accuracy: 0.5588 - val_loss: 0.7135 - val_accuracy: 0.6259\n",
      "Epoch 34/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7160 - accuracy: 0.5635 - val_loss: 0.9874 - val_accuracy: 0.6235\n",
      "Epoch 35/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7160 - accuracy: 0.5744 - val_loss: 0.7274 - val_accuracy: 0.6249\n",
      "Epoch 36/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5633 - val_loss: 0.7577 - val_accuracy: 0.6214\n",
      "Epoch 37/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7160 - accuracy: 0.5694 - val_loss: 1.0143 - val_accuracy: 0.6214\n",
      "Epoch 38/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5710 - val_loss: 1.0101 - val_accuracy: 0.6228\n",
      "Epoch 39/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5695 - val_loss: 0.7393 - val_accuracy: 0.6175\n",
      "Epoch 40/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7160 - accuracy: 0.5897 - val_loss: 0.9840 - val_accuracy: 0.6231\n",
      "Epoch 41/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5728 - val_loss: 0.9006 - val_accuracy: 0.6238\n",
      "Epoch 42/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5641 - val_loss: 0.7921 - val_accuracy: 0.6259\n",
      "Epoch 43/100\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 0.7159 - accuracy: 0.5683 - val_loss: 0.8240 - val_accuracy: 0.6235\n",
      "Epoch 44/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5596 - val_loss: 0.8383 - val_accuracy: 0.6235\n",
      "Epoch 45/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5683 - val_loss: 0.8342 - val_accuracy: 0.6235\n",
      "Epoch 46/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5696 - val_loss: 1.0276 - val_accuracy: 0.6224\n",
      "Epoch 47/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5624 - val_loss: 0.8599 - val_accuracy: 0.6238\n",
      "Epoch 48/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5679 - val_loss: 0.7988 - val_accuracy: 0.6217\n",
      "Epoch 49/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5733 - val_loss: 0.9180 - val_accuracy: 0.6252\n",
      "Epoch 50/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5708 - val_loss: 0.7796 - val_accuracy: 0.6235\n",
      "Epoch 51/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5787 - val_loss: 0.7991 - val_accuracy: 0.6210\n",
      "Epoch 52/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5697 - val_loss: 0.7594 - val_accuracy: 0.6245\n",
      "Epoch 53/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5791 - val_loss: 0.7429 - val_accuracy: 0.6263\n",
      "Epoch 54/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5592 - val_loss: 0.7779 - val_accuracy: 0.6235\n",
      "Epoch 55/100\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 0.7159 - accuracy: 0.5771 - val_loss: 0.7187 - val_accuracy: 0.6189\n",
      "Epoch 56/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5593 - val_loss: 0.7604 - val_accuracy: 0.6210\n",
      "Epoch 57/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5770 - val_loss: 0.9320 - val_accuracy: 0.6235\n",
      "Epoch 58/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5764 - val_loss: 0.7755 - val_accuracy: 0.6235\n",
      "Epoch 59/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5737 - val_loss: 0.8019 - val_accuracy: 0.6217\n",
      "Epoch 60/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5596 - val_loss: 0.8623 - val_accuracy: 0.6224\n",
      "Epoch 61/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5698 - val_loss: 0.7707 - val_accuracy: 0.6235\n",
      "Epoch 62/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5765 - val_loss: 0.7802 - val_accuracy: 0.6186\n",
      "Epoch 63/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5761 - val_loss: 0.6717 - val_accuracy: 0.6322\n",
      "Epoch 64/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5791 - val_loss: 0.7104 - val_accuracy: 0.6252\n",
      "Epoch 65/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5770 - val_loss: 0.9087 - val_accuracy: 0.6238\n",
      "Epoch 66/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5668 - val_loss: 0.7891 - val_accuracy: 0.6238\n",
      "Epoch 67/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5912 - val_loss: 0.8946 - val_accuracy: 0.6231\n",
      "Epoch 68/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5787 - val_loss: 0.9628 - val_accuracy: 0.6203\n",
      "Epoch 69/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5668 - val_loss: 0.9359 - val_accuracy: 0.6256\n",
      "Epoch 70/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5882 - val_loss: 0.8749 - val_accuracy: 0.6235\n",
      "Epoch 71/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5756 - val_loss: 0.7448 - val_accuracy: 0.6224\n",
      "Epoch 72/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5774 - val_loss: 0.7563 - val_accuracy: 0.6235\n",
      "Epoch 73/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5877 - val_loss: 0.7050 - val_accuracy: 0.6231\n",
      "Epoch 74/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5686 - val_loss: 0.9266 - val_accuracy: 0.6252\n",
      "Epoch 75/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5933 - val_loss: 0.7875 - val_accuracy: 0.6238\n",
      "Epoch 76/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5653 - val_loss: 1.0315 - val_accuracy: 0.6217\n",
      "Epoch 77/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5894 - val_loss: 0.9669 - val_accuracy: 0.6242\n",
      "Epoch 78/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5843 - val_loss: 0.9291 - val_accuracy: 0.6210\n",
      "Epoch 79/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5743 - val_loss: 1.0502 - val_accuracy: 0.6210\n",
      "Epoch 80/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5798 - val_loss: 0.7013 - val_accuracy: 0.6259\n",
      "Epoch 81/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5811 - val_loss: 0.7445 - val_accuracy: 0.6249\n",
      "Epoch 82/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5848 - val_loss: 0.9057 - val_accuracy: 0.6231\n",
      "Epoch 83/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5894 - val_loss: 0.9597 - val_accuracy: 0.6231\n",
      "Epoch 84/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.6005 - val_loss: 0.9065 - val_accuracy: 0.6224\n",
      "Epoch 85/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5940 - val_loss: 0.7702 - val_accuracy: 0.6217\n",
      "Epoch 86/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5881 - val_loss: 0.8233 - val_accuracy: 0.6189\n",
      "Epoch 87/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7158 - accuracy: 0.5882 - val_loss: 0.7124 - val_accuracy: 0.6228\n",
      "Epoch 88/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5952 - val_loss: 0.7427 - val_accuracy: 0.6207\n",
      "Epoch 89/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5796 - val_loss: 0.9773 - val_accuracy: 0.6210\n",
      "Epoch 90/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7158 - accuracy: 0.6016 - val_loss: 0.9507 - val_accuracy: 0.6217\n",
      "Epoch 91/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.5830 - val_loss: 1.0521 - val_accuracy: 0.6235\n",
      "Epoch 92/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7158 - accuracy: 0.5878 - val_loss: 1.0020 - val_accuracy: 0.6221\n",
      "Epoch 93/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7158 - accuracy: 0.5930 - val_loss: 0.9953 - val_accuracy: 0.6217\n",
      "Epoch 94/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7158 - accuracy: 0.5962 - val_loss: 0.8002 - val_accuracy: 0.6249\n",
      "Epoch 95/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7158 - accuracy: 0.5959 - val_loss: 0.8501 - val_accuracy: 0.6242\n",
      "Epoch 96/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7158 - accuracy: 0.5902 - val_loss: 0.7715 - val_accuracy: 0.6242\n",
      "Epoch 97/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7158 - accuracy: 0.5989 - val_loss: 0.9858 - val_accuracy: 0.6224\n",
      "Epoch 98/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7158 - accuracy: 0.6068 - val_loss: 0.8647 - val_accuracy: 0.6221\n",
      "Epoch 99/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7158 - accuracy: 0.5973 - val_loss: 0.8806 - val_accuracy: 0.6235\n",
      "Epoch 100/100\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7158 - accuracy: 0.6092 - val_loss: 1.0256 - val_accuracy: 0.6231\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 1.0256 - accuracy: 0.6231\n"
     ]
    }
   ],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=X_train.shape[1]))\n",
    "    model.add(Dense(96, activation='selu',\n",
    "          kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l1_l2(0.1, 0.2)))\n",
    "#     model.add(Dropout(0.4))\n",
    "#     model.add(Dense(96, activation='relu',\n",
    "#           kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.05)))\n",
    "#     model.add(Dropout(0.6))\n",
    "#     model.add(Dense(96, activation='relu',\n",
    "#           kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.05)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    optim = keras.optimizers.SGD(learning_rate=0.001969)\n",
    "    model.compile(optimizer=optim,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model()\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=256, validation_data=(x_2023, y_2023))\n",
    "test_loss, test_acc = model.evaluate(x_2023, y_2023)\n",
    "# # model = KerasClassifier(model=create_model, epochs=100, batch_size=32, validation_)\n",
    "# model.fit(X_train, y_train)\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# opti_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "# print(\"Trening\")\n",
    "# print(opti_report)\n",
    "# opti_pre = opti_report['weighted avg']['precision']\n",
    "# opti_f1 = opti_report['weighted avg']['f1-score']\n",
    "# opti_auc = metrics.roc_auc_score(y_test, model.predict(X_test))\n",
    "\n",
    "\n",
    "\n",
    "# pred_val = model.predict(x_2023)\n",
    "# opti_val_report = classification_report(y_2023, pred_val, output_dict=True)\n",
    "# print(\"Walidacja\")\n",
    "# print(opti_val_report)\n",
    "# opti_pre_val = opti_val_report['weighted avg']['precision']\n",
    "# opti_f1_val = opti_val_report['weighted avg']['f1-score']\n",
    "# opti_auc_val = metrics.roc_auc_score(y_2023, model.predict(x_2023))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# filename = 'results.json'\n",
    "\n",
    "# final_results = {\n",
    "#     \"prediction_basic\": basic_pre,\n",
    "#     \"v_prcision_basic\": basic_pre_val,\n",
    "#     \"prediction_optimized\": opti_pre,\n",
    "#     \"v_prcision_optimized\": opti_pre_val,\n",
    "#     \"f1_score_opt\": opti_f1,\n",
    "#     \"f1_score_opt_val\": opti_f1_val,\n",
    "#     \"auc_opt\": float(opti_auc),\n",
    "#     \"auc_opt_val\": float(opti_auc_val),\n",
    "# }\n",
    "\n",
    "\n",
    "# with open(filename, 'r') as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "\n",
    "# data['Sztuczna sieć neuronowa'] = (final_results)\n",
    "\n",
    "\n",
    "# with open('results.json', 'w') as file:\n",
    "#     json.dump(data, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
