{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import RFECV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Standard\n",
    "# df = pd.read_csv('../db/out/wta_std.csv')\n",
    "\n",
    "# mean\n",
    "# df = pd.read_csv('../db/out/wta_mean.csv')\n",
    "\n",
    "# pvp\n",
    "# df = pd.read_csv('../db/out/wta_pvp.csv')\n",
    "\n",
    "# mean + pvp\n",
    "df = pd.read_csv('../db/out/wta_mean_pvp.csv')\n",
    "\n",
    "\n",
    "# df = pd.read_csv('../db/clean_and_no_NaN/match_stats_with_wins.csv')\n",
    "\n",
    "# df = pd.read_csv('../db/clean_and_no_NaN/wta_matches_clean.csv')\n",
    "\n",
    "# df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ustalanie symetrycznosci cech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testp1 = df.filter(regex='_P1_vs$', axis=1)\n",
    "\n",
    "testp2 = df.filter(regex='_P2_vs$', axis=1)\n",
    "\n",
    "\n",
    "feats = list(zip(testp1.columns, testp2.columns))\n",
    "\n",
    "\n",
    "for col_a, col_b in feats:\n",
    "    df[col_a + '_diff'] = df[col_a] - df[col_b]\n",
    "\n",
    "df.drop(testp1.columns, axis=1, inplace=True)\n",
    "\n",
    "df.drop(testp2.columns, axis=1, inplace=True)\n",
    "\n",
    "non_numeric_columns = df.select_dtypes(['object']).columns\n",
    "\n",
    "df = df.drop(non_numeric_columns, axis=1)\n",
    "\n",
    "testp1 = df.filter(regex='_P1$', axis=1)\n",
    "\n",
    "testp2 = df.filter(regex='_P2$', axis=1)\n",
    "\n",
    "feats = list(zip(testp1.columns, testp2.columns))\n",
    "\n",
    "for col_a, col_b in feats:\n",
    "    df[col_a + '_diff'] = df[col_a] - df[col_b]\n",
    "\n",
    "df.drop(testp1.columns, axis=1, inplace=True)\n",
    "\n",
    "df.drop(testp2.columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizacja danych\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric_columns = df.select_dtypes(['object']).columns\n",
    "\n",
    "df_numeric_only = df.drop(non_numeric_columns, axis=1)\n",
    "\n",
    "df = df_numeric_only\n",
    "\n",
    "df = df.drop(['match_id'], axis=1)\n",
    "\n",
    "# for deccision tree plot\n",
    "# df = df[['rank_points_P1_diff', 'win_percentage_P1_diff',\n",
    "#          'surface_wins_P1_diff', 'y']]\n",
    "\n",
    "\n",
    "# for svm plot\n",
    "df = df[['surface_wins_P1_diff', 'win_percentage_P1_diff', 'y']]\n",
    "\n",
    "\n",
    "df = df.fillna(df.median())\n",
    "\n",
    "\n",
    "def normalize(dff):\n",
    "    result = dff.copy()\n",
    "    for feature_name in dff.columns:\n",
    "        max_value = dff[feature_name].max()\n",
    "        min_value = dff[feature_name].min()\n",
    "        result[feature_name] = (\n",
    "            dff[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result\n",
    "\n",
    "\n",
    "df = normalize(df)\n",
    "# df.info()\n",
    "df = df.fillna(df.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przygotowanie danych do modelu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = pd.DataFrame(df['y'])\n",
    "df = df.drop(['y'], axis=1)\n",
    "X = df\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=18, stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression Basic hiperparameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(random_state=14, max_iter=1000)\n",
    "\n",
    "result = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "# cnf_matrix\n",
    "\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix)\n",
    "# disp.plot()\n",
    "# plt.show()\n",
    "\n",
    "y_pred_proba = result.predict_proba(X_test)[::, 1]\n",
    "fpr_basic, tpr_basic, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc_basic = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "# plt.plot(fpr_basic, tpr_basic, label=\"Logisitic Regression, auc=\"+str(auc_basic))\n",
    "# plt.legend(loc=4)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regresion with RandomizedSearchCV\n",
    "model = LogisticRegression()\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.logspace(0, 4, num=10),\n",
    "    'solver': ['liblinear', 'sag', 'newton-cg']\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(model, param_grid, n_iter=10,\n",
    "                            cv=10, scoring='accuracy', n_jobs=-1, random_state=1, verbose=2)\n",
    "\n",
    "result = search.fit(X_train, y_train)\n",
    "\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "y_pred_proba = result.predict_proba(X_test)[::, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regresion with RFECV\n",
    "\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "model = LogisticRegression(C=464, solver='sag', random_state=14)\n",
    "cv = StratifiedKFold(3)\n",
    "\n",
    "rfecv = RFECV(model, cv=cv, scoring='accuracy', step=1)\n",
    "\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfecv.predict(X_test)\n",
    "\n",
    "print('Optimal number of features : %d' % rfecv.n_features_)\n",
    "\n",
    "ranks = pd.DataFrame(\n",
    "    rfecv.ranking_, index=X.columns, columns=['Rank'])\n",
    "\n",
    "print(ranks.sort_values(by='Rank', ascending=True))\n",
    "\n",
    "# rfecv.support_rfecv_df = pd.DataFrame(rfecv.ranking_, index=X.columns, columns=[\n",
    "#                                       'Rank']).sort_values(by='Rank', ascending=True)\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.xlabel('Number of features selected')\n",
    "plt.ylabel('Cross validation score (nb of correct classifications)')\n",
    "plt.plot(range(1, len(rfecv.cv_results_[\n",
    "         \"mean_test_score\"]) + 1), rfecv.cv_results_[\"mean_test_score\"])\n",
    "plt.title(\"Recursive Feature Elimination \\nwith correlated features\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "# cnf_matrix\n",
    "\n",
    "\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix)\n",
    "# disp.plot()\n",
    "# plt.grid(False)\n",
    "# plt.show()\n",
    "\n",
    "y_pred_proba = result.predict_proba(X_test)[::, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc), color='red')\n",
    "plt.plot(fpr_basic, tpr_basic, label=\"Logisitic Regression, auc=\" +\n",
    "         str(auc_basic), color='blue')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Basic hiperparameters\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from graphviz import Source\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "result = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "# cnf_matrix\n",
    "\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix)\n",
    "# disp.plot()\n",
    "# plt.show()\n",
    "\n",
    "y_pred_proba = result.predict(X_test)\n",
    "fpr_basic, tpr_basic, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc_basic = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "# plt.plot(fpr_basic, tpr_basic, label=\"Basic, auc=\"+str(auc))\n",
    "# plt.legend(loc=4)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree ferature importance plot\n",
    "\n",
    "\n",
    "model = DecisionTreeClassifier(\n",
    "    max_depth=7, max_features='log2', criterion='entropy')\n",
    "\n",
    "\n",
    "result = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "feat_importances = pd.DataFrame(\n",
    "    model.feature_importances_, index=X_test.columns, columns=[\"Importance\"])\n",
    "feat_importances.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "feat_importances.plot(kind='bar', figsize=(8, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree with RandomizedSearchCV\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': list(range(5, 20, 1)),\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'criterion': ['gini', 'entropy', 'log_loss']\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(model, param_grid, n_iter=15,\n",
    "                            cv=10, scoring='accuracy', n_jobs=-1, random_state=1, verbose=2)\n",
    "\n",
    "# search = RFE(model, n_features_to_select=5, step=1)\n",
    "\n",
    "# execute search\n",
    "result = search.fit(X_train, y_train)\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)\n",
    "\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "best_random = result.best_estimator_\n",
    "y_pred_test = best_random.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_test)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_test)\n",
    "plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc))\n",
    "plt.plot(fpr_basic, tpr_basic, label=\"Basic, auc=\" +\n",
    "         str(auc_basic), color='blue')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree with RFECV\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "\n",
    "model = DecisionTreeClassifier(\n",
    "    max_depth=7, max_features='log2', criterion='entropy')\n",
    "\n",
    "cv = StratifiedKFold(3)\n",
    "\n",
    "rfecv = RFECV(model, cv=cv, scoring='accuracy', step=1)\n",
    "\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfecv.predict(X_test)\n",
    "\n",
    "print('Optimal number of features : %d' % rfecv.n_features_)\n",
    "\n",
    "# print(rfecv.feature_names_in_)\n",
    "\n",
    "# ranks = pd.DataFrame(\n",
    "#     rfecv.ranking_, index=X.columns, columns=['Rank'], sorted=True)\n",
    "\n",
    "# export_graphviz(rfecv.estimator_, out_file='tree.dot', feature_names=ranks[:rfecv.n_features_],\n",
    "#                 class_names=['0', '1'], rounded=True, filled=True)\n",
    "\n",
    "Source.from_file('tree.dot')\n",
    "\n",
    "selected_features = X_train.columns[rfecv.support_]\n",
    "\n",
    "print(selected_features)\n",
    "\n",
    "\n",
    "ranks = pd.DataFrame(\n",
    "    rfecv.ranking_, index=X.columns, columns=['Rank'])\n",
    "\n",
    "ranks_1 = ranks[ranks['Rank'] == 1]\n",
    "\n",
    "print(ranks_1)\n",
    "\n",
    "print(ranks.sort_values(by='Rank', ascending=True))\n",
    "\n",
    "# rfecv.support_rfecv_df = pd.DataFrame(rfecv.ranking_, index=X.columns, columns=[\n",
    "#                                       'Rank']).sort_values(by='Rank', ascending=True)\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.xlabel('Number of features selected')\n",
    "plt.ylabel('Cross validation score (nb of correct classifications)')\n",
    "plt.plot(range(1, len(rfecv.cv_results_[\n",
    "         \"mean_test_score\"]) + 1), rfecv.cv_results_[\"mean_test_score\"])\n",
    "plt.title(\"Recursive Feature Elimination \\nwith correlated features\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix\n",
    "\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix)\n",
    "disp.plot()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "y_pred_proba = result.predict_proba(X_test)[::, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc))\n",
    "plt.plot(fpr_basic, tpr_basic, label=\"Basic, auc=\" +\n",
    "         str(auc_basic), color='blue')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# everyting apllied\n",
    "from sklearn.tree import export_graphviz\n",
    "model = DecisionTreeClassifier(\n",
    "    max_depth=7, max_features='log2', criterion='entropy')\n",
    "\n",
    "\n",
    "result = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "export_graphviz(model, out_file='tree.dot',\n",
    "                feature_names=X_train.columns, class_names=['0', '1'], rounded=True, filled=True)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Source\n",
    "Source.from_file('tree.dot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC Basic hiperparameters\n",
    "from sklearn import svm\n",
    "\n",
    "model = svm.LinearSVC()  # Linear Kernel\n",
    "\n",
    "result = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "y_pred_proba = result.predict(X_test)\n",
    "# fpr_basic, tpr_basic, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "# auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "# plt.plot(fpr_basic, tpr_basic, label=\"Basic, auc=\"+str(auc), color='red')\n",
    "# plt.legend(loc=4)\n",
    "# plt.show()\n",
    "\n",
    "# cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "# cnf_matrix\n",
    "\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix)\n",
    "# disp.plot()\n",
    "# plt.grid(False)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Linear SVC with RandomizedSearchCV\n",
    "\n",
    "# model = svm.LinearSVC()  # Linear Kernel\n",
    "\n",
    "# param_grid = {'C': range(1, 100, 5),\n",
    "#               'loss': ['hinge', 'squared_hinge'],\n",
    "#               }\n",
    "\n",
    "\n",
    "# search = RandomizedSearchCV(model, param_grid, n_iter=5,\n",
    "#                             cv=10, scoring='accuracy', n_jobs=-1, random_state=1)\n",
    "\n",
    "# # execute search\n",
    "# result = search.fit(X_train, y_train)\n",
    "# # summarize result\n",
    "# print('Best Score: %s' % result.best_score_)\n",
    "# print('Best Hyperparameters: %s' % result.best_params_)\n",
    "\n",
    "\n",
    "# best_random = result.best_estimator_\n",
    "# y_pred_test = best_random.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "# fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_test)\n",
    "# auc = metrics.roc_auc_score(y_test, y_pred_test)\n",
    "# plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc))\n",
    "# plt.legend(loc=4)\n",
    "# plt.show()\n",
    "\n",
    "# cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "# cnf_matrix\n",
    "\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix)\n",
    "# plt.grid(False)\n",
    "# disp.plot()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC with RFECV\n",
    "model = svm.LinearSVC(C=6, loss='hinge')\n",
    "\n",
    "cv = StratifiedKFold(3)\n",
    "\n",
    "rfecv = RFECV(model, cv=cv, scoring='accuracy', step=1, random_state=14)\n",
    "\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfecv.predict(X_test)\n",
    "\n",
    "print('Optimal number of features : %d' % rfecv.n_features_)\n",
    "\n",
    "ranks = pd.DataFrame(\n",
    "    rfecv.ranking_, index=X.columns, columns=['Rank'])\n",
    "\n",
    "print(ranks.sort_values(by='Rank', ascending=True))\n",
    "\n",
    "# rfecv.support_rfecv_df = pd.DataFrame(rfecv.ranking_, index=X.columns, columns=[\n",
    "#                                       'Rank']).sort_values(by='Rank', ascending=True)\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.xlabel('Number of features selected')\n",
    "plt.ylabel('Cross validation score (nb of correct classifications)')\n",
    "plt.plot(range(1, len(rfecv.cv_results_[\n",
    "         \"mean_test_score\"]) + 1), rfecv.cv_results_[\"mean_test_score\"])\n",
    "plt.title(\"Recursive Feature Elimination \\nwith correlated features\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix\n",
    "\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix)\n",
    "disp.plot()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "y_pred_proba = result.predict(X_test)\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc), color='blue')\n",
    "plt.plot(fpr_basic, tpr_basic, label=\"Basic, auc=\"+str(auc), color='red')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "model = svm.LinearSVC(C=6, loss='hinge')\n",
    "\n",
    "result = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "y_pred_proba = result.predict(X_test)\n",
    "\n",
    "\n",
    "plot_decision_regions(X_train, y_train, clf=model, legend=2)\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# # Plotting our two-features-space\n",
    "# sns.scatterplot(x=X_train[:, 0], y=X_train[:, 1],\n",
    "#                 hue=y_train,\n",
    "#                 s=8)\n",
    "# # Constructing a hyperplane using a formula.\n",
    "# w = model.coef_[0]           # w consists of 2 elements\n",
    "# b = model.intercept_[0]      # b consists of 1 element\n",
    "# x_points = np.linspace(-1, 1)    # generating x-points from -1 to 1\n",
    "# y_points = -(w[0] / w[1]) * x_points - b / \\\n",
    "#     w[1]  # getting corresponding y-points\n",
    "# # Plotting a red hyperplane\n",
    "# plt.plot(x_points, y_points, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Legacy] Passive Aggressive Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passive Aggressive Classifier Basic hiperparameters\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "\n",
    "model = PassiveAggressiveClassifier(average=True, random_state=75)\n",
    "\n",
    "\n",
    "result = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix)\n",
    "disp.plot()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "y_pred_proba = result.predict(X_test)\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passive Aggressive Classifier with RandomizedSearchCV\n",
    "\n",
    "model = PassiveAggressiveClassifier()\n",
    "\n",
    "param_grid = {'C': range(1, 100, 1)\n",
    "              }\n",
    "\n",
    "\n",
    "search = RandomizedSearchCV(model, param_grid, n_iter=5,\n",
    "                            cv=10, scoring='accuracy', n_jobs=-1, random_state=1)\n",
    "\n",
    "# execute search\n",
    "result = search.fit(X_train, y_train)\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)\n",
    "\n",
    "\n",
    "best_random = result.best_estimator_\n",
    "y_pred_test = best_random.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_test)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_test)\n",
    "plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passive Aggressive Classifier with RFECV\n",
    "\n",
    "model = PassiveAggressiveClassifier(C=result.best_params_['C'])\n",
    "\n",
    "cv = StratifiedKFold(3)\n",
    "\n",
    "rfecv = RFECV(model, cv=cv, scoring='accuracy', step=1)\n",
    "\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfecv.predict(X_test)\n",
    "\n",
    "print('Optimal number of features : %d' % rfecv.n_features_)\n",
    "\n",
    "ranks = pd.DataFrame(\n",
    "    rfecv.ranking_, index=X.columns, columns=['Rank'])\n",
    "\n",
    "print(ranks.sort_values(by='Rank', ascending=True))\n",
    "\n",
    "# rfecv.support_rfecv_df = pd.DataFrame(rfecv.ranking_, index=X.columns, columns=[\n",
    "#                                       'Rank']).sort_values(by='Rank', ascending=True)\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.xlabel('Number of features selected')\n",
    "plt.ylabel('Cross validation score (nb of correct classifications)')\n",
    "plt.plot(range(1, len(rfecv.cv_results_[\n",
    "         \"mean_test_score\"]) + 1), rfecv.cv_results_[\"mean_test_score\"])\n",
    "plt.title(\"Recursive Feature Elimination \\nwith correlated features\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix\n",
    "\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix)\n",
    "disp.plot()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "y_pred_proba = result.predict(X_test)\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
